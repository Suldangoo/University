# 1주차

### 인공지능 정의

- 인공지능은 인간의 **학습**능력, **추론**능력, **지각**능력을 인공적으로 구현하려는 컴퓨터 과학의 세부분야 중 하나이다.
- AI - 머신 러닝 - 딥 러닝 - 파운데이션 모델 - LLM 순으로 내부 개념
    - 머신 러닝 : SVM, Boost
    - 딥 러닝 : YOLO
    - 파운데이션 모델 : DALLE, SORA, Stable Diffusion
    - 라지 랭귀지 모델 : GPT, LLaMA, Gemini

# 2주차

## 인공지능의 역사

- 1950, 컴퓨터의 시초인 **튜링 머신**을 **앨런 튜링**이 개발
    - 튜링 테스트 : 기계가 인간과 얼마나 유사한 응답을 내는지 확인하기 위한 테스트
- 1956, **존 매카시**가 **인공지능**이라는 용어를 처음으로 등장
- 1957, **프랑크 로젠블랫**이 인공 신경망 모형의 하나인 **퍼셉트론**의 등장
    - 인간의 뇌에 있는 신경 세포인 뉴런에서 영감을 받은 모델
    - 입력 값에 **가중치**를 곱하고, **활성화 함수**를 통과시켜 출력값을 내는 구조

```bash
가중치 : 각 입력 신호가 결과에 미치는 영향력을 조절하는 역할.
예를 들어 스팸과 스팸이 아닌 메일을 분류할 때, 스팸 메일에 특정 단어 (ex 할인)
가 많이 들어있다면 그 단어에 높은 가중치를 부여할 수 있음

학습 : 이 가중치 값은 AI가 학습을 시작하는 최초엔, 모든 단어에 랜덤한 가중치나 같은 값을 두고
계속해서 데이터를 학습하며 스팸 메일에서 자주 보이는 단어가 있다면 점점 가중치가 조정되는 것.
초기 가중치를 사용해 스팸 여부를 예측한 후, 실제 데이터(스팸인가 아닌가)를 비교함
이후 맞다면 그 값을 강화하고, 틀렸다면 오류를 측정하여 가중치를 조정함
이러한 가중치 조정 과정은 일반적으로 경사 하강법과 같은 최적화 알고리즘을 사용하여 수행됨

경사 하강법 : 모델의 예측이 실제 값과 얼마나 차이나는지 계산하고 (오류가 얼마나 되는지)
오류를 최소하하는 방향으로 가중치를 조금씩 변경하는 것.
이 때 각 단계에서 가중치 변경량은 오류의 '기울기'에 비례한다.

활성화 함수 : 가중치가 적용된 입력값들의 합을 받아 임계값을 기준으로 0 or 1 출력하는 함수
```

- 초기 컴퓨터는 bit 연산으로 전기가 흐르는지, 안흐르는지로 데이터를 처리함
- 대표적인 연산자로 NOT, AND, OR, XOR이 있는데, 퍼셉트론으로 XOR을 해결하지 못함
- 1986, **제프리 힌튼**이 **다층 퍼셉트론**(MLP)를 발명
    - 퍼셉트론에 하나 이상의 **은닉층**을 추가해 XOR문제를 해결, **오차 역전파**
    - 은닉층은 입력층과 출력층 사이에 위치해 복잡한 패턴이나 데이터 관계를 깊이 학습함

```bash
은닉층 : 은닉층은 데이터의 다양한 특징을 추출하고, 조합하는 역할을 한다.
입력 데이터의 다양한 특징을 학습하고, 다음 층으로 전달한다.
예를 들어 얼굴 인식이라면 첫 번째 은닉층은 각도와 구도를, 두 번째 은닉층은 눈, 코, 입을 구분

오차 역전파 : 다층 퍼셉트론 학습 과정 중 사용되는 기술.
네트워크를 통해 얻은 출력값의 오류를 기반으로 각 가중치를 효율적으로 조정한다.
네트워크의 출력층에서 발생한 오류를 입력층 방향으로 거꾸로 전파시켜 각 층의 가중치를 조정
예를 들어 고양이를 개로 잘못 분류했을 때, 이 오류를 계산하고 다시 출력층부터 시작하여
입력층까지 거꾸로 오류를 전파하고 각 층의 가중치가 오류를 줄이는 방향으로 조정되는 것.
```

- 2012, ILSVRC 이미지 분류 대회에서 **제프리 힌튼**이 **CNN** 기반 모델인 AlexNet으로 1위 차지
- 2014, **GANs**의 등장으로 인공지능 기반 생성 모델의 잠재력 확인
    - 서로 다른 목적을 가진 두 네트워크의 적대적 학습이 특징인 모델
- 2017, **Transformer**의 등장으로 자연어 분류 및 생성 기술에도 급격한 발전
    - Attention is All You Need 논문에서 첫 발표
    - 인코더 - 디코더 형식으로 구성되어 있으며, ChatGPT도 Transformer 기반 기술
- 2024, CES에서 소개된 IT 트렌드인 On-device AI가 등장
    - 네트워크 연결 없이 반도체 칩만으로 AI 모델을 구동시키는 기술
    - 퀄컴, 인텔을 중심으로 반도체 기술 개발중이며 삼성전자가 S24에 탑재

## 인공지능 분야 기술용어

- 규칙 기반 인공지능
    - 학습이 불필요한 가장 기초적인 형태의 인공?지능
    - 그냥 정해진 버튼을 누르면 정해진 임무를 수행하는 단순한 기계
- 학습 기반 인공지능
    - 머신러닝, 딥러닝이 학습기반 기술에 포함되어있음
    - 데이터를 기반으로 특성을 추출하고, 분류를 통해 결과를 출력
    - 여기서 머신러닝은 인간이 직접 추출하고, 딥러닝은 컴퓨터가 처리함
- 정형 데이터 : 정해진 틀 / 형태에 맞게 저장된 데이터 (SQL, 스프레드시트)
- 반정형 데이터 : 데이터 모델을 따르지는 않지만 구조화된 데이터 (JSON, XML, Email)
- 비정형 데이터 : 특정 형태나 데이터 모델 없이 저장된 데이터 (Audio, Video, Documents)
- Zero-shot Learning : 학습하지 않은 범주의 데이터 분류
- One-shot Learning : 1개의 샘플 데이터만 학습해 사용하는 모델
- Few-shot Learning : N개의 데이터를 학습해 사용하는 모델
- Classification (클래시피케이션, 분류, 이미지 분류, 오디오 분류, 텍스트 분류)
    - 데이터를 미리 정의된 카테고리로 분류하는 것, 디지털
- Regression (리그레션, 회귀, 수치 예측)
    - 데이터에 기반해 연속적인 수치(Value) 를 예측, 아날로그
- Pattern Recognition (인식, 얼굴인식, 지문인식, 음성인식)
    - 데이터에서 패턴이나 규칙을 식별, 한 데이터에서 패턴을 찾아내기
- Attribute Classification (속성 분류)
    - 객체의 여러 속성을 분류하여 정보를 구분, 여러 속성들을 분류하기
- Clustering (클러스터링, 군집화)
    - 비슷한, 유사한 특성을 가진 데이터들이 뭉친 포인트들을 그룹화하는 것
- Detection (객체 검출 = 위치 + 정보 분류)
    - 객체의 위치를 찾고, 그 객체가 어떤건지 분류, YOLO
- Localization (상세 위치 파악)
    - 정보의 정확한 위치를 식별하고, 픽셀 단위로 세그멘테이션
- Generation (생성)
    - 이미지, 텍스트, 오디오, 비디오 등 데이터를 생성
- Translation (변환, t2t, i2i, t2i, tts)
    - 생성 기술중 한 가지 방식으로, 한 형식의 데이터를 다른 형식으로 변환
- Supervised Learning (지도학습)
    - 데이터들과 레이블(답)을 학습하고, 이후 데이터들 중 한 가지 데이터를 주면 레이블 분류
- Semi-Supervised Learning (반지도학습)
    - 데이터들과 레이블(답)을 학습하되 주는 데이터는 레이블에 없는 데이터. 그래도 분류함
- Self-Supervised Learning (자기지도학습)
    - 이전 정보로부터 다음 단어를 예측하고, Mask 영역 또한 예측할 수 있게 됨
- Unsupervised Learning (비지도학습)
    - 레이블을 주지 않아도, 데이터들을 주면 패턴을 분석해 비슷한 패턴을 가진 데이터끼리 분류
- Reinforcement Learning (강화학습)
    - 무언가 목표에 가까울 수록 좋은 보상을 주며, 좋은 보상을 얻는 방향으로 학습하는 방식
    - 특정 상태에서 어떤 행동을 취하는 것이 가장 최적인지 학습. 동물의 학습 능력 모방
- Continual Learning (연속학습)
    - 기존 데이터를 학습한 상태에서 새로운 데이터를 추가로 학습, 또 다음 새로운 데이터를 추가로 학습을 반복하며, 지속적으로 새로운 데이터를 학습하는 방법
    - 실제 환경에선 새로운 데이터가 지속적으로 발생하니, 이 방식으로 딥러닝 문제점 보완
    - 단 커다란 문제점인 치명적 망각 (앞 훈련 내용을 잊어버리는 것) 이 있음
- Pre-train (사전학습) : 대용량 데이터를 기반으로 모델 학습.
    - 대규모 데이터셋을 사용해 모델을 맨 처음부터 학습시키는 과정
    - 일반적인 지식을 학습하며, 다양한 종류의 정보를 처리할 수 있다.
    - 그냥 밑바탕 지식 알아두기이다. 단어 의미, 문장 구조, 문맥 등을 학습한다.
    - 이는 매우 범용적이라 특정한 작업에 사용하기엔 추가적인 학습이 필요하다.
- Fine-Tuning (파인 튜닝) : 대용량 데이터 기반 모델 활용, 소규모 데이터로 목적에 맞게 모델 수정 및 재학습하는 과정.
    - 사전학습된 모델 구조를 대부분 유지하며 목표 작업에 특화되도록 추가 학습을 하는 과정
    - 전이 학습은 사전 학습된 모델을 새로운 작업에 재활용하는 기술. 모델이 기존에 학습한 지식을 새로운 상황에 적용할 수 있도록 도와주는 효율적인 학습법이다.
    - 모델의 ‘헤드’는 보통 네트워크의 마지막 부분이다. 모델 대부분의 층(몸통)은 그대로 두고, 마지막 층인 헤드만 특정 작업에 맞게 변경하는 것이다. 이러면 기존 지식을 유지하고 새로운 작업에 효과적으로 적응할 수 있다. 미세조정한다는 것이다.
- Split (데이터 분할)
    - 인공지능 모델 개발 시 주어진 데이터를 쪼개어 학습용, 테스트용으로 분리하는 것.
    - 비율은 정해지지 않았으며, 학습/검증/테스트로 구분하는 경우도 있음.
- K-fold-cross-validation
    - 데이터를 k분할하여 교차검증하는 것.
    - 데이터를 분할하고 학습하면 운이 나쁠 시 특정 방향으로만 일반화하게 학습할 수 있음
    - 모든 데이터에 대해 학습 / 테스트를 하면 그런 단점을 보완할 수 있음
- 모델 학습
    - 데이터로부터 모델을 학습하고, 이를 통해 예측 및 결정을 할 수 있는 기술의 집합
    - y = f(x)에서 함수 f(x)는 모델, f를 알아내는 과정을 학습, f로부터 y를 얻는 과정을 예측
    - 모델 학습은 데이터 수집 → 모델 선택 → 학습 → 예측(추론)으로 구성된다.

## 퍼셉트론

- 입력층 : 입력값을 받아들이는 층
- 출력층 : 퍼셉트론의 출력값을 나타내는 층 (1 또는 0)
- 노드 : 입력값의 가중합과 활성화를 결정하는 층
    - 두 입력값에 각각 가중치를 곱해 더한 뒤, 편향을 더하고 활성화 함수 통과
    - 퍼셉트론을 하나만 사용하면 단층 신경망, 여러 개 사용하면 다층 신경망
- 활성화 함수
    - 임계값을 기준으로 노드의 출력값을 결정. 층을 쌓는 효과가 생긴다.
    - 퍼셉트론에서 출력값을 다음 퍼셉트론으로 넘기는 과정에 변환 적용하는 함수
    - 모델의 복잡도를 높이기 위해 사용한다.
    - 여러 함수들이 있는데, 대표적으로 **계단(Step)**, **시그모이드**와 **ReLU**가 있다.
- 결정 경계
    - 가중치 합 + 편향 = w1x1 + w2x2 + … + b
    - 퍼셉트론을 돌려 나온 값을 그래프에 적용해보면 **대각선**이 생긴다.
    - 이 대각선보다 작은 값은 0으로 분류하고, 큰 값은 1로 분류하게 된다.
    - 여기서 대각선, 모델이 데이터를 두 클래스로 나누는 경계 자체를 결정 경계라고 함
    - 활성화 함수의 형태가 결정경계의 형태를 변경하지 않는다.
- 단층 퍼셉트론은 한 개의 결정 경계만 지니기 때문에 선형적으로 분리 가능한 문제만 적합하지만 XOR같은 비선형 문제의 경우엔 단순히 하나의 직선으로는 구분할 수 없다.

## 다층 퍼셉트론

- 다층 퍼셉트론은 여러 개의 은닉층을 포함할 수 있으며 각 은닉층은 여러 개의 뉴런으로 구성될 수 있다.
- 이 각각의 뉴런은 입력에 대해 독립적인 가중치와 편향을 가지고 각각의 결정 경계를 형성한다.
- 그래서 다층 퍼셉트론은 여러 개의 결정 경계를 가질 수 있고, 비선형 문제를 해결 가능하다.
- 각 노드에 서로 다른 가중치와 편향을 적용해 값이 나오고, 중간값과 편향을 사용해 최종 출력을 더한다. 입력 층에 0 0 / 0 1 / 1 0 / 1 1을 넣어 XOR 문제를 해결할 수 있다.

### 손실 함수

- Loss Function는 실제로 모델이 예측한 결과와 정답 사이의 오차를 측정하는 함수이다.
- 나온 결과값에 미분 연산을 통해서 기울기를 계산하고, 기울기가 크면 오차가 큰 것읻 .
- 이 함수를 사용해 모델의 성능을 평가하고, 모델의 가중치를 조정한다.
- 가중치가 조절되면, 결정 경계 또한 변하게 된다.
- 일반적으로 오차가 적은 경계가 더 좋은 결정 경계이다.
- 초기에는 MSE, **평균 제곱 오차** 손실 함수가 널리 쓰였다.
    - 회귀 문제에서 사용되며, 단순 이진 분류일 때 쓰인다.
    - 모델 예측 값과 실제 값 사이의 제곱 차이의 평균을 계산한다.
- 이후엔 교차 엔트로피 (Cross-Entropy) 손실함수가 쓰였다.
    - 분류 문제에서 사용되며, 단순 이진 분류가 아닐 때 쓰인다.
    - 실제 데이터의 확률 분포와 학습 모델이 계산한 확률 분포의 차이를 최소화하는 방식이다.
    - 모델의 예측이 실제 클래스 레이블을 얼마나 잘 반영하는지를 측정한다.
- 이 함수 리턴값을 최소화함으로서 모델은 데이터에 잘 맞도록 조정되며, 최적의 결정경계를 찾게 된다.

### 엔트로피와 크로스 엔트로피

- 엔트로피 : 확률 분포 P(x)에 대한 불확실성의 척도
    - 정보량이 낮다 = 특별하지 않다 = 엔트로피가 낮다 = 구분이 쉽다
    - 정보량이 높다 = 특별하다 = 엔트로피가 높다 = 구분이 어렵다
    - 어떤 사건의 발생 가능성이 매우 예측 가능하다면 (확률이 높다면) 그 사건의 엔트로피는 낮다. 반대로 발생 가능성이 매우 예측이 어렵다면 그 사건의 엔트로피는 높다.
    - 만약 앞뒷면이 50%로 정확히 공정한 동전을 던지게 되면, 이는 예측하기 가장 어려우며 정보량도 가장 높다. 공식을 사용하면 1비트가 나오며, 엔트로피가 제일 높다.
    - 그러나 편향되어 앞면이 나올 확률이 90%인 동전이 있다면 예측이 더 쉬워지므로 엔트로피가 낮다. 공식을 사용햐면 약 0.47비트가 나오며, 엔트로피가 비교적 낮다.
- 크로스 엔트로피 : 실제 정답 분포와 모델이 예측한 분포간 차이
    - CE가 낮다 = 두 분포의 차이가 적다 = 모델이 실제 답을 잘 예측한다.

## 최적 해 계산

- **경사 하강법**과 **오차 역전파**를 통해 최적의 값을 찾는다.
- **순전파 → 손실 계산 → 역전파(그라디언트 계산 포함)** 순으로 진행된다.

```bash
순전파 : 입력층부터 출력층까지 순차적으로 각 층의 뉴런을 통과하며 최종 출력을 계산하는 과정
```

### 오차 역전파

- 정답과 신경망이 예측한 값과의 오차를 최소화하는 가중치를 찾는 알고리즘.
- 미분의 연쇄 법칙을 이용해 출력층에 가까운 가중치부터 수정해나간다.
    - 결론적으로 신경망의 모든 가중치를 조정한다.
- 가중치에 손실 함수에서 나온 기울기(오류의 차이)값을 빼주면 새로운 가중치 값 획득
- 순전파를 통해 얻은 값과 실제 값 차이를 손실 함수로 계산, 이 값을 최소화하는 것이 목표
- 비선형 구조 활성화 함수가 필요한데, 선형 구조면 미분 결과 기울기가 아닌 상수를 출력함.

```bash
그라디언트 : 각 가중치에 대한 손실 함수의 미분값들을 벡터로 표현한 정보
손실 함수만으로는 가중치를 어떻게 변경해야 할지 정보가 담겨져 있지 않은데,
이 그라디언트를 손실 함수의 값으로 계산해내면 가중치를 최적화할 정보가 된다.

그라디언트 계산 : 손실 함수의 결과를 바탕으로 출력층 -> 입력층 거꾸로 가며
각 뉴런의 가중치에 대한 손실 함수의 그라디언트(미분값)를 계산한다.
각 층의 가중치가 손실에 얼마나 영향을 미쳤는지를 평가한다.
그라디언트(기울기)를 계산하고, 그라디언트가 가리키는 방향의 반대로 가중치를 조정해야 함

가중치 업데이트 : 계산된 그라디언트를 사용해 각 가중치를 조정한다.
이 때 학습률(Learning Rate)을 곱하여 그라디언트의 크기를 조절한다.
```

### 경사 하강법

- 시작점부터 최적화된 가중치를 얻기 위해 출발하는 학습 여정
- 함수의 기울기(경사)를 구하고, 경사의 반대 방향으로 계속 이동시키며 최솟값에 이를 때까지 반복하는 학습 방법
- 시작점부터 다음 지점까지 가는 거리를 보폭이라고 함
    - 보폭은 **학습률(Learning Rate)**과 기울기의 곱으로 표현된다.
    - 즉, 기울기가 클 수록 보폭도 커지고, 기울기가 작으면 보폭도 작아진다.
    - 학습률이 너무 크면 보폭이 너무 커버려서 수렴 속도가 느려진다.
    - 학습률이 너무 작으면 보폭이 너무 작아버려서 수렴 속도가 느려진다.
    - 적당한 학습률을 설정해야 수렴 속도가 최적화된다.
- 경사 하강법의 과정이자 AI 학습  과정
    1. 초기화 : 가중치를 초기 값으로 설정. 무작위 값일 수도 있고, 0일 수도 있고, 특별한 전략에 따른 값일 수도 있다.
    2. 순전파 : 입력 데이터를 신경망에 통과시켜 예측 값 획득
    3. 손실 계산 : 예측 값과 실제 데이터의 차이를 손실 함수를 통해 계산
    4. 그라디언트 계산 : 오차 역전파를 통해 각 가중치에 대한 손실 함수의 그라디언트를 계산
    5. 가중치 업데이트 : 계산된 그라디언트를 사용해 각 가중치를 업데이트. 이 때 학습률을 곱해 가중치 업데이트의 크기를 조절
    6. 위 과정을 손실이 충분히 작아질 때까지 반복한다.

# 3주차

## 인공지능 분류모델 RNN

- Recurrent Neural Network
- **시계열 데이터**나 **순차 데이터**를 분류하는 대표 모델
    - 시계열 데이터 : 텍스트, 음성, 생체신호 등 연속적인 형태로 존재하는 데이터
    - 즉 Classification(분류)와 Regression(회귀)를 하는 분류 모델
- 네트워크 내에서 순환하는 연결 구조를 갖고 있어 시간에 따라 변화하는 데이터의 내부 상태를 유지할 수 있다. 이 특성 덕분에 이전 정보를 기억하고, 현재 입력과 결합해 출력을 생성한다.
    - 연속학습의 일종이기 때문에, 장기 의존성 문제로 초기 정보를 후반부에 잊어버린다.
- 하나의 Unit에 반복적으로 접근하기에, 데이터의 길이가 길어지면 초기 입력 정보를 잊어버린다.
- LSTM과 GRU가 등장해 기억력 문제를 어느정도 해결했으며, 잃어버릴 메모리와 기억할 메모리를 구분하여 반복 사용한다.
- 여러 계층 활용 시 동일한 유닛에 반복 접근하며, 분류 단계에선 마지막에 MLP를 사용한다.

## 인공지능 분류모델 CNN

- Convolution Neural Network
- **이미지 분류**의 대표 모델
- 컨볼루션 연산(합성곱)으로 이루어진 합성곱 층과 활성화 함수 등으로 이루어짐
- 분류 시에는 마지막에 MLP 구조를 활용
- 입력 → 합성곱 → ReLU → 합성곱 → ReLU → 맥스풀링 → 출력

### 커널, 필터

- 커널은 이미지로부터 특징을 추출하기 위해 가중치를 행렬로 나타낸 것
- 커널의 집합을 필터라고도 부름
- 커널의 이동거리는 **스트라이드**라고 함
- 커널의 합성곱 결과로부터 얻어지는 이미지는 **특징 맵**이라고 함
- 커널의 크기만큼 있는 이미지를 한 픽셀의 특징으로 압축하는거라, 이미지가 작아짐

### 지역특징과 전역특징

- CNN 모델의 앞 부분에서 추출한 특징일 수록, **지역 특징정보**를 학습
- CNN 모델의 뒷 부분에서 추출한 특징일 수록, **전역 특징정보**를 학습
- 앞 부분은 화질이 좋아 세부적인 특징을 보고, 뒷 부분은 화질이 흐려 전체적인 모습을 본다.

### 차원 축소 (Pooling)

- 차원을 축소한다는 것은, 중요 특징만을 남겨두고 이미지 크기를 줄이는 것
- 주로 활용되는 풀링은 **Max Pooling**
    - 정의된 커널 내에서, 가장 큰 값만 남기기
- 이 외에도 평균, 최소 풀링이 존재하기는 함

### 평탄화(Flatten)

- MLP층의 입력에 이미지를 넣기 위해서는 반드시 1차원 벡터 형태여야 함
- 즉, 이미지를 **1차원 벡터로 변환**하는 작업

### 기타 CNN 기술 단어

- 데이터 증강 : 이미지에 여러 변형을 주어 이미지 개수를 늘리는 것. 회전, 크기 변경, 밀림, 반사, 이동 등을 사용해 서 있는 사람이나 거꾸로 매달린 사람이나 사람으로 인식하게끔 하는 것.
- 크롭핑 : 이미지의 일정 부분을 도려내기. 불필요한 부분 잘라내기
- 패딩 : 특정 영역을 0 혹은 아무 값으로 채우기. 0으로 채우는 것은 제로 패딩이라고 함
- 플립 : 대칭 회전이라고도 함. 이미지를 좌우나 상하로 뒤집는 것
- 정규화 : 데이터의 분포를 정규분포의 형태로 바꿔주는 것.
    - 어느 특정 이미지가 특정 색깔이 많다면, 해당 이미지의 특징보다 색에 집중해버릴 수 있어서 RGB 분포를 전부 똑같게 바꿔주는 작업을 의미한다.
- 오버피팅 : 머신러닝, 딥러닝에서 자주 발생하는 문제. 모델이 학습 데이터에 너무너무 잘 맞춰져있어 학습 데이터는 정답률이 매우 높지만 새로운 데이터나 검증 데이터는 성능이 좋지 않은 현상을 의미한다. 즉, 범용성이 떨어진다.
- 드롭아웃 : 학습 과정에서 네트워크의 일부 연결을 임의로 끊어서 모델이 특정 노드에 과도하게 의존하는 것을 방지하는 것.

## 인공지능 분류모델 Transformer

- Attention is all you need (NeurlPS 2017) 논문에서 첫 발표
- 시계열 데이터의 **순서정보와 무관**하게, 일괄적으로 정보를 처리할 수 있는 구조
    - Positional Encoding (PE) : 순서정보와 상관 없이 한 번에 입력을 받는 것
- **인코더 → 디코더**의 구조로 이루어져 있음
    - 인코더는 주로 분류에 활용되고, 디코더는 주로 생성에 활용됨
- 이미지 분류 시에도, 지역/전역 특징에 대해서 구분 성능이 뛰어난 Vision Transformer(ViT)

## 여러 인공지능 모델

### 인공지능 인식모델

- 가짜 얼굴인식, 스피치 인식 등 작업별 기술이 개별적으로 발전
- 스피치 인식(ASR)의 경우, **트랜스포머의 인코더만 활용하는 BERT** 기반 모델이 증가

### 인공지능 분할모델

- 이미지 분할(Segmentation)은 도로의 객체 분할, 이미지 편집 분야에 활용됨
- 동영상 객체 분할은 연속적인 프레임 내에서 동일객체 분리를 위한 작업
- 인공지능 분할 모델엔 SAM이나 Detectron2 등이 있음

### 인공지능 생성모델

- GANs : 생성자와 감별자의 적대적 학습 기반 생성기술
    - 생성자가 최대한 가짜 이미지를 반들고, 감별자는 진짜와 가짜를 최대한 구별해냄
    - 감별자는 진짜 이미지가 1이 나오도록, 가짜 이미지가 0이 나오도록 학습
    - 생성자는 가짜 이미지를 생성해서 감별자의 출력이 1이 나오도록 학습
- Stable Diffusion : 이미지 ↔ 노이즈간 단계적 변환을 통한 생성기술
    - 음성, 이미지, 텍스트 데이터 모두에 적용 가능
    - Forward Diffusion Process : 이미지에 노이즈를 추가하는 학습
    - Reverse Diffusion Process : Noise로부터 데이터를 복원하는 과정 학습
- 인공지능 생성 모델이 얼마나 진짜같은지 평가하는 것은 FID
    - Frechlet Inception Distance
    - 원본 이미지 데이터 셋과 생성된 이미지 데이터 셋 사이의 통계적 거리 측정