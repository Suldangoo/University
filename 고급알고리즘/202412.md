# 14주차

## 선형 회귀 (Linear Regression)

- H(x) = Wx + b
- 데이터가 좌표상에 6개 있을 때, 그 데이터들을 대표하는 직선 하나를 찾는 것이 코딩의 목표
- 점 두 개를 지나는 직선은 단 하나 존재.
- 점 세개를 지나는 직선은 하나만 있거나, 한 개도 없을 수 있다.
- 특정 데이터들을 대표하는 직선이 만들어졌다면, 인풋 하나에 아웃풋이 하나 나오는 함수가 만들어진다. ( f(x) = y )
- 컴퓨터가 해당 직선을 구하기 위해선, 모든 직선들을 전부 그어 점들과의 거리가 최소가 되는 직선을 찾는다.
    - 그러나 모든 선을 전부 다 무한하게 체크할 수는 없으므로, 이 횟수를 최소화하는 알고리즘이 존재한다.
    - 두 점 사이의 거리를 구하는건 피타고라스의 정리로 구할 수 있다.
- 우리가 하고자 하는 것은 직선을 찾는, 일차식을 찾는 것이다.
    - 1차식을 구하기 위해선 2차식을 풀어야하고, 2차식을 구하기 위해선 3차식을 풀어야 한다.
- 우리의 목표는 1차식의 계수이다.
    - Y = Wx + b가 있을 때, b(상수항)은 중요하지 않다.
    - H(x) = Wx를 구하는 것. 직선의 기울기만 구하면 된다.
    - (0, 0)을 지나는 직선에서, 데이터들의 분포에 따른 일정 범위 내를 컴퓨터의 성능에 따라 해당 바운더리 안에서 브루트포스할 수 있다.
- 그렇게 모든 선들의 거리를 구하면, 거리들을 점으로 찍을 경우 2차방정식이 그려진다.
    - 따라서 2차방정식의 부분이 그려졌다면, 해당 포인트들로 2차방정식을 모델링
    - 거기서 최솟값으로 추정되는 부분을 조사하여 찾을 수 있다. 단, 이 경우 근삿값을 찾는 것이기에 값이 정확하게 나오지 않을 수 있다.
- 이렇게 다음 데이터를 예측할 수 있는 선형 회귀를 구하는 것을, 통계학에서 회귀 분석이라고 한다.
- 선형 회귀 (1차식) 을 배우면 회귀 분석 (2차식, 3차식…)을 할 수 있게 된다.
- 1차 선형 회귀를 했다면 아래 두 가지를 볼 수 있다.
    - 변수를 늘려 다중 선형 회귀로 직선 여러개를 만들기
    - 차수를 늘려 2차 선형 회귀로 곡선 만들기

## 변수를 늘린 다중 선형 회귀

- 변수에 나이, 인종, 성별 등등 여러가지 파라미터를 넣는 것에도 선형 회귀가 가능하다.
- 공부 시간과 점수라는 변수 두 개가 있을 때, 시간에 따른 시험 성적 예측 직선을 예상할 수 있다.
    - 그러나 이 경우, 공부를 더 많이 했는데 점수가 더 낮은 데이터도 존재할 수 있다.
- 따라서 공부 태도 등의 변수를 추가한다. 이런 변수가 많이 추가될 수록 점점 정확도가 높아진다.
    - 데이터가 무조건 많은게 좋은건 아니다. 예측하고자 하는 데이터에 관련이 높은 데이터들이 모인 것이 더욱 정확하다.
- 10p ~ 14p 스킵
- Y = w1x1 + w2x2 + w3x3… 등이 있을 때, 변수(x1, 예를 들어 나이)에 따라 웨이트, 가중치를 별도로 정할 수 있다. 더 중요한 변수에 더 많은 가중치를 주는 것이다.
- Y = WX가 있을 때, W를 구하고싶다면 역행렬 계산을 하면 된다.
- Y * X ^ -1 = W로, Y와 X를 알고 있어야 한다.
- 하나의 곡선에서 최저점을 찾기 위해 STEP을 뛸 때, 최대한 빨리 최저점을 찾기 위한 최적의 STEP을 찾기 위해선 A 모멘텀, B 무작위가 필요하다.
    - 더 최저점을 가기 위한 방향을 정하는 것이 모멘텀
    - 얼마나 뛸지에 대한 거리를 무작위로 둔다
- 보기 좋고, 계산이 빠르게 돌아갈 수 있도록 하기 위해 정규화 공식, 표준화 공식이 있다.
    - 우선 Zero-centerd로 바꾸어 상수항을 제거
    - 이후 데이터를 정규화하여 데이터의 범위가 0~1안으로 전부 들어오도록 함
- 수능에서 표준점수를 사용하는 이유는, 과탐/사탐에서 서로 다른 과목의 점수를 비교하기 위함이다.
    - 물리가 어렵고 화학이 쉬웠다면, 두 시험의 같은 점수는 같은 가치가 아니다.
- 변수가 다른 그래프에서의 연관관계성, 호환성을 위해 표준화를 사용한다.

| Original Data | 너무 분포가 제각각 |
| --- | --- |
| Normalized Data | 어느정도 단위를 맞추었으나 기준이 다름 |
| Standardized Data | 비교가 쉬워짐 |

## 이차곡선 회귀 분석

- 약간 틀려도 적당한 선을 긋는게 좋은데, 너무 완벽하게 데이터에 맞는 직선을 그어버리면 표본에선 정답률이 좋으나 일반적인 상황에선 오히려 오답이 많이 나온다.

# 15주차

## 선형회귀

- 훈련, 검증, 테스트는 6:2:2 등등의 비율로 분할하는 느낌
- 하나의 직선으로는 OR, AND를 나눌 수 있지만, XOR은 나눌 수 없다.
- 이 XOR 문제를, 뉴럴 네트워크와 순전파, Convex 곡선으로 해결한다.
    - 즉, 함수를 2개, 혹은 그 이상을 써서 구분할 수 있다.
- 즉, XOR를 해결하기 위해선 입력층 / 은닉층 / 출력층이 필요하게 된다.
- weight를 조절하는, 학습 방법엔 순전파와 역전파가 존재한다.
- 배니싱 그라디언트를 쓰면 인풋의 가중치가 갈수록 사라진다.
    - 그라디언트가 소실되지 않는 새로운 활성함수 사용
    - 층이 너무 많을 수록 오버피티잉 자주 발생
- 오버피팅을 방지하기 위해서 드롭아웃으로 길 몇개를 끊고, 중요하지 않은 파라미터를 비활성화할 수도 있다.
- 학습을 모니터링하며 오버피팅과 나빠지는 그 경계 사이에서 학습을 중지하고, 그걸 모델로서 체크포인트를 두고 사용하게 됨

## 컨볼루전

- Convolution, 합성곱
- 두 함수의 곱이며, 이전의 값, 현재의 값, 이후의 값을 고려한 곱

### 1차원 컨볼루전

- 종을 쳤을 때 울리는 소리의 곡선 함수
- 종을 여러 번 때릴 때 해당 초에만 어느 세기로 치는지 적은 함수
- 두 개를 합성곱하여, 소리가 어떻게 날지 구하는 것이 합성곱
- 함수 두 개를 과거부터 지나가며, 다른 함수와 겹치는 영역을 재고, 그 면적만큼 새로운 C 함수를 그리는 것
- [3, 6, 9, 6, 5, 4, 3, 2, 1]
- [0, 0, 0, 2, 0, 0, 1, 0, 0]
- 밑에있는 막대를 왼쪽 끝부터 오른쪽 끝까지 이동시키며 합성곱한다.
    - 단, 1차원 행렬은 반드시 두 배열 중 하나를 반전시켜야 한다.

### 2차원 컨볼루전

- 두 개의 사이즈가 다른 행렬을, 서로 곱하고 합치는 과정
- 모든 속성의 값이 1인 2*2 + 모든 속성의 값이 2인 3*3 을 합성곱하면
    - 스트라이드(이동거리)는 1이다.

| 2 | 4 | 4 | 2 |
| --- | --- | --- | --- |
| 4 | 8 | 8 | 4 |
| 4 | 8 | 8 | 4 |
| 2 | 4 | 4 | 2 |

### 블러 필터

- 어떤 이미지 연산에 모든 속성이 1/9인 3*3 (9픽셀) 을 합성곱하는것
- 브러시  크기를 키우면 모든 속성이 1/25인 5*5 (25픽셀)
- 피부는 블러필터, 눈 등에는 엣지 디텍션으로 뚜렷하게, 이런 식으로 이미지를 처리하여 실제 서비스하는 필터를 개발하는 것

### 뉴런의 학습방식

- 고기를 본다는 자극과 종소리를 듣는다는 자극이 계속되면, 그 자극때의 뉴런과 뉴런 사이의 강도가 강해져 침을 흘리는 파블로프의 개 현상이 나온다.
- 마지막 PPT는 대부분 넘어가다가 필터링에서 끝남
    - 로우 패스 필터 : 저주파 통과
    - 하이 패스 필터 : 고주파 통과
    - 원하는 주파수만을 걸러내는 것을 필터링이라고 한다