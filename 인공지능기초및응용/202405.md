# 드림 부스

- 적은 이미지만 가지고도 해당 객체를 생성하는 생성 모델 학습 기술
- 소규모 데이터의 특징을 유지하고, 다양한 상황의 이미지 생성

# **FairDeDup**

- 공정성에 따른 연구를 하고 있는 논문
- 영상과 언어를 같이 다루는 모델에 대한 내용
- 대용량 데이터의 50%만 활용하며 정확도는 유지하고 공정성을 높이는 기술
- 데이터 특징 추출, 클러스터링, 유사한 데이터끼리 그룹화
- 데이터 Prune(가지치기) 활용. 데이터 및 모델의 일부를 삭제하는 방법

### 관련 연구

- Vision-Language Fairness
    - VLP 모델에 성별, 나이, 인종에 따른 Bias가 존재한다는 연구 결과가 있음
    - 특정 인종이나 성별이 학습 데이터에 많으면 편향이 됨

### 기존 기술의 문제점

- Web-scale은 라지 모델 학습을 할 때 824개의 GPU를 11일 동안 사용
- AWS EC2로 학습하는 경우 1회 학습에 $870 소모
- 그러나 Web 데이터에는 복제본이 많이 존재함.
- 데이터를 크롤링하는 과정에서 파일명만 다르고 파일은 같은 데이터가 받아질 수 있다.
- 복제 데이터를 삭제하는 과정에서 Bias 문제가 강화됨
    - SemDeDup같은 복제본 삭제 연구는 Fairness 측면이 전혀 진행되어있지 않음

### 기존 방법

- CLIP 모델로 데이터의 특징 추출 후 유사 데이터 그룹화 (클러스터링)
- 같은 군집 안에서 유사 데이터 또 그룹화 (이웃)
    - 이건 클러스터링이 아니라 feature를 하나하나 비교하며 임계치 기준으로 이웃 결정
- 클래스의 중심으로부터 가장 먼 샘플만  남기며, 유사 데이터가 없는 경우엔 보존
    - 중심에서 가장 먼 것은 가장 독특한 샘플일 것으로 가정
    - 그러나 SubGroup을 고려하지 않은 삭제로, 특정 Group의 샘플이 버려지는 경우 발생

### 제안 방법

- FairDeDup은 CLIP 모델로 데이터 특징 추출 후 유사 데이터 그룹화 (클러스터링)
- 같은 군집 안에서 유사 데이터 또 그룹화 (여기까지 동일)
- 클러스터 내 샘플과 SubGroup간 유사도 평균 계산
- 임의의 샘플을 선택한 후 이웃 내 모든 샘플과 SubGroup간 Consine 유사도 측정
- **가장 낮은 평균 유사도**를 보이는 SubGroup과 **가장 높은 유사도**를 가지는 샘플을 보존
- SubGroup을 고려하므로 Data Balance 유지
    - 예를 들어 간호사같은 경우, 여자가 압도적으로 많으므로, 가장 특이한 (남자 간호사) 데이터를 보존하는 방식

### 결과

- 공정성 측면에서 정확도 차이가 SemDeDup 대비 향상된 결과를 보임
- 공정성을 측정하기 위해 만들어진 FairFace, FACET 데이터에 대해서도 평가해보니 기존 방법에 비해 낮은 Bias(편향) 이 나왔으며 전체 데이터를 사용한 Full과도 유사하거나 더 좋은 결과를 보임

### 한계점

- 기존 데이터를 50% 축소시키는 과정에서 어떤 이유로 공정성에 문제가 발생하는지 명확한 근거와 이유를 제시하고 있지 않음.
- 클러스터링 과정에서 이미 성별이나 인종이 구분된 경우엔 클러스터 내 SubGroup 간 다양성 개선이 도움이 되지 않음.
- CLIP등 특징 추출 모델이 편향된 경우에 대해 추가 연구 필요
- 웹 기반 데이터와 특정 VLP에만 초점을 맞추고 있어 일반화 성능이 보장되지 않음.

# CLIP

- 이미지와 텍스트 Pair로부터 특징을 학습
- Contrastive Language-Image Pre-training
    - Contrastive : 대조적인, AI 관련 기술에 자주 등장
- 비전과 랭귀지 두 가지 정보를 한 번에 고려해서 학습
- 메소드
    - 사진을 학습할 때 메인 사진을 앵커로 두고, 흑백으로 전환한 것을 포지티브, 전혀 다른 이미지를 네거티브 이미지로 둔다.
    - 인코더를 통해 특징 벡터를 각각의 이미지에 대해 따로 얻게 된다.
    - 앵커-포지티브는 거리가 좁아지게, 앵커-네거티브는 거리가 멀어지게 조정한다.
- ImageNet은 25000명의 레이블 작업자가 필요해 불가능
- Natural Language Supervision 연구에전 적은 데이터를 활용했음
- 따라서, 인터넷으로 데이터를 수집하고 제안하여 데이터 부족 문제를 해결
- Contrastive Pre-training (대조 사전학습)
    - 텍스트 인코더에서 특징 벡터를 모두 뽑아 열로 둠
    - 이미지 인코더에서 특징 벡터를 모두 뽑아 행으로 둠
    - 이러면 NxN Pair 맵이 만들어짐
    - 여기서 서로 같은 Pair가 (같은 행, 열 번호) Positive pair
    - 그 외의 나머지 Pair들이 Nagative pair
- Test는 Zero-shot 이미지 분류를 통해 평가
    - 텍스트 인코더를 두고, 이미지 인코더를 통과시켜 낸 특징 벡터 중 가장 높은 시밀러리티를 낸 것을 테스트
    - 이 덕분에 여러 논문에서 CLIP을 사용할 정도로 확실한 Zero-Shot 테스트 냄
- Diffusion 기반의 T2I 모델의 이미지/텍스트 인코더에 활용됨
- 한계점
    - 일반적인 분류 기술(Zero-shot 아닌 기술) SOTA와 비교 시 낮은 성능
    - OCR Representation에 대해서 성능이 낮음
        - 텍스트를 인식하는 기술에 대해서 성능이 낮음, 일반화 관점에서의 한계점