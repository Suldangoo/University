# 1주차

### 인공지능 정의

- 인공지능은 인간의 **학습**능력, **추론**능력, **지각**능력을 인공적으로 구현하려는 컴퓨터 과학의 세부분야 중 하나이다.
- AI - 머신 러닝 - 딥 러닝 - 파운데이션 모델 - LLM 순으로 내부 개념
    - 머신 러닝 : SVM, Boost
    - 딥 러닝 : YOLO
    - 파운데이션 모델 : DALLE, SORA, Stable Diffusion
    - 라지 랭귀지 모델 : GPT, LLaMA, Gemini

# 2주차

## 인공지능의 역사

- 1950, 컴퓨터의 시초인 **튜링 머신**을 **앨런 튜링**이 개발
    - 튜링 테스트 : 기계가 인간과 얼마나 유사한 응답을 내는지 확인하기 위한 테스트
- 1956, **존 매카시**가 **인공지능**이라는 용어를 처음으로 등장
- 1957, **프랑크 로젠블랫**이 인공 신경망 모형의 하나인 **퍼셉트론**의 등장
    - 인간의 뇌에 있는 신경 세포인 뉴런에서 영감을 받은 모델
    - 입력 값에 **가중치**를 곱하고, **활성화 함수**를 통과시켜 출력값을 내는 구조

```bash
가중치 : 각 입력 신호가 결과에 미치는 영향력을 조절하는 역할.
예를 들어 스팸과 스팸이 아닌 메일을 분류할 때, 스팸 메일에 특정 단어 (ex 할인)
가 많이 들어있다면 그 단어에 높은 가중치를 부여할 수 있음

학습 : 이 가중치 값은 AI가 학습을 시작하는 최초엔, 모든 단어에 랜덤한 가중치나 같은 값을 두고
계속해서 데이터를 학습하며 스팸 메일에서 자주 보이는 단어가 있다면 점점 가중치가 조정되는 것.
초기 가중치를 사용해 스팸 여부를 예측한 후, 실제 데이터(스팸인가 아닌가)를 비교함
이후 맞다면 그 값을 강화하고, 틀렸다면 오류를 측정하여 가중치를 조정함
이러한 가중치 조정 과정은 일반적으로 경사 하강법과 같은 최적화 알고리즘을 사용하여 수행됨

경사 하강법 : 모델의 예측이 실제 값과 얼마나 차이나는지 계산하고 (오류가 얼마나 되는지)
오류를 최소하하는 방향으로 가중치를 조금씩 변경하는 것.
이 때 각 단계에서 가중치 변경량은 오류의 '기울기'에 비례한다.

활성화 함수 : 가중치가 적용된 입력값들의 합을 받아 임계값을 기준으로 0 or 1 출력하는 함수
```

- 초기 컴퓨터는 bit 연산으로 전기가 흐르는지, 안흐르는지로 데이터를 처리함
- 대표적인 연산자로 NOT, AND, OR, XOR이 있는데, 퍼셉트론으로 XOR을 해결하지 못함
- 1986, **제프리 힌튼**이 **다층 퍼셉트론**(MLP)를 발명
    - 퍼셉트론에 하나 이상의 **은닉층**을 추가해 XOR문제를 해결, **오차 역전파**
    - 은닉층은 입력층과 출력층 사이에 위치해 복잡한 패턴이나 데이터 관계를 깊이 학습함

```bash
은닉층 : 은닉층은 데이터의 다양한 특징을 추출하고, 조합하는 역할을 한다.
입력 데이터의 다양한 특징을 학습하고, 다음 층으로 전달한다.
예를 들어 얼굴 인식이라면 첫 번째 은닉층은 각도와 구도를, 두 번째 은닉층은 눈, 코, 입을 구분

오차 역전파 : 다층 퍼셉트론 학습 과정 중 사용되는 기술.
네트워크를 통해 얻은 출력값의 오류를 기반으로 각 가중치를 효율적으로 조정한다.
네트워크의 출력층에서 발생한 오류를 입력층 방향으로 거꾸로 전파시켜 각 층의 가중치를 조정
예를 들어 고양이를 개로 잘못 분류했을 때, 이 오류를 계산하고 다시 출력층부터 시작하여
입력층까지 거꾸로 오류를 전파하고 각 층의 가중치가 오류를 줄이는 방향으로 조정되는 것.
```

- 2012, ILSVRC 이미지 분류 대회에서 **제프리 힌튼**이 **CNN** 기반 모델인 AlexNet으로 1위 차지
- 2014, **GANs**의 등장으로 인공지능 기반 생성 모델의 잠재력 확인
    - 서로 다른 목적을 가진 두 네트워크의 적대적 학습이 특징인 모델
- 2017, **Transformer**의 등장으로 자연어 분류 및 생성 기술에도 급격한 발전
    - Attention is All You Need 논문에서 첫 발표
    - 인코더 - 디코더 형식으로 구성되어 있으며, ChatGPT도 Transformer 기반 기술
- 2024, CES에서 소개된 IT 트렌드인 On-device AI가 등장
    - 네트워크 연결 없이 반도체 칩만으로 AI 모델을 구동시키는 기술
    - 퀄컴, 인텔을 중심으로 반도체 기술 개발중이며 삼성전자가 S24에 탑재

## 인공지능 분야 기술용어

- 규칙 기반 인공지능
    - 학습이 불필요한 가장 기초적인 형태의 인공?지능
    - 그냥 정해진 버튼을 누르면 정해진 임무를 수행하는 단순한 기계
- 학습 기반 인공지능
    - 머신러닝, 딥러닝이 학습기반 기술에 포함되어있음
    - 데이터를 기반으로 특성을 추출하고, 분류를 통해 결과를 출력
    - 여기서 머신러닝은 인간이 직접 추출하고, 딥러닝은 컴퓨터가 처리함
- 정형 데이터 : 정해진 틀 / 형태에 맞게 저장된 데이터 (SQL, 스프레드시트)
- 반정형 데이터 : 데이터 모델을 따르지는 않지만 구조화된 데이터 (JSON, XML, Email)
- 비정형 데이터 : 특정 형태나 데이터 모델 없이 저장된 데이터 (Audio, Video, Documents)
- Zero-shot Learning : 학습하지 않은 범주의 데이터 분류
- One-shot Learning : 1개의 샘플 데이터만 학습해 사용하는 모델
- Few-shot Learning : N개의 데이터를 학습해 사용하는 모델
- Classification (클래시피케이션, 분류, 이미지 분류, 오디오 분류, 텍스트 분류)
    - 데이터를 미리 정의된 카테고리로 분류하는 것, 디지털
- Regression (리그레션, 회귀, 수치 예측)
    - 데이터에 기반해 연속적인 수치(Value) 를 예측, 아날로그
- Pattern Recognition (인식, 얼굴인식, 지문인식, 음성인식)
    - 데이터에서 패턴이나 규칙을 식별, 한 데이터에서 패턴을 찾아내기
- Attribute Classification (속성 분류)
    - 객체의 여러 속성을 분류하여 정보를 구분, 여러 속성들을 분류하기
- Clustering (클러스터링, 군집화)
    - 비슷한, 유사한 특성을 가진 데이터들이 뭉친 포인트들을 그룹화하는 것
- Detection (객체 검출 = 위치 + 정보 분류)
    - 객체의 위치를 찾고, 그 객체가 어떤건지 분류, YOLO
- Localization (상세 위치 파악)
    - 정보의 정확한 위치를 식별하고, 픽셀 단위로 세그멘테이션
- Generation (생성)
    - 이미지, 텍스트, 오디오, 비디오 등 데이터를 생성
- Translation (변환, t2t, i2i, t2i, tts)
    - 생성 기술중 한 가지 방식으로, 한 형식의 데이터를 다른 형식으로 변환
- Supervised Learning (지도학습)
    - 데이터들과 레이블(답)을 학습하고, 이후 데이터들 중 한 가지 데이터를 주면 레이블 분류
- Semi-Supervised Learning (반지도학습)
    - 데이터들과 레이블(답)을 학습하되 주는 데이터는 레이블에 없는 데이터. 그래도 분류함
- Self-Supervised Learning (자기지도학습)
    - 이전 정보로부터 다음 단어를 예측하고, Mask 영역 또한 예측할 수 있게 됨
- Unsupervised Learning (비지도학습)
    - 레이블을 주지 않아도, 데이터들을 주면 패턴을 분석해 비슷한 패턴을 가진 데이터끼리 분류
- Reinforcement Learning (강화학습)
    - 무언가 목표에 가까울 수록 좋은 보상을 주며, 좋은 보상을 얻는 방향으로 학습하는 방식
    - 특정 상태에서 어떤 행동을 취하는 것이 가장 최적인지 학습. 동물의 학습 능력 모방
- Continual Learning (연속학습)
    - 기존 데이터를 학습한 상태에서 새로운 데이터를 추가로 학습, 또 다음 새로운 데이터를 추가로 학습을 반복하며, 지속적으로 새로운 데이터를 학습하는 방법
    - 실제 환경에선 새로운 데이터가 지속적으로 발생하니, 이 방식으로 딥러닝 문제점 보완
    - 단 커다란 문제점인 치명적 망각 (앞 훈련 내용을 잊어버리는 것) 이 있음
- Pre-train (사전학습) : 대용량 데이터를 기반으로 모델 학습.
    - 대규모 데이터셋을 사용해 모델을 맨 처음부터 학습시키는 과정
    - 일반적인 지식을 학습하며, 다양한 종류의 정보를 처리할 수 있다.
    - 그냥 밑바탕 지식 알아두기이다. 단어 의미, 문장 구조, 문맥 등을 학습한다.
    - 이는 매우 범용적이라 특정한 작업에 사용하기엔 추가적인 학습이 필요하다.
- Fine-Tuning (파인 튜닝) : 대용량 데이터 기반 모델 활용, 소규모 데이터로 목적에 맞게 모델 수정 및 재학습하는 과정.
    - 사전학습된 모델 구조를 대부분 유지하며 목표 작업에 특화되도록 추가 학습을 하는 과정
    - 전이 학습은 사전 학습된 모델을 새로운 작업에 재활용하는 기술. 모델이 기존에 학습한 지식을 새로운 상황에 적용할 수 있도록 도와주는 효율적인 학습법이다.
    - 모델의 ‘헤드’는 보통 네트워크의 마지막 부분이다. 모델 대부분의 층(몸통)은 그대로 두고, 마지막 층인 헤드만 특정 작업에 맞게 변경하는 것이다. 이러면 기존 지식을 유지하고 새로운 작업에 효과적으로 적응할 수 있다. 미세조정한다는 것이다.
- Split (데이터 분할)
    - 인공지능 모델 개발 시 주어진 데이터를 쪼개어 학습용, 테스트용으로 분리하는 것.
    - 비율은 정해지지 않았으며, 학습/검증/테스트로 구분하는 경우도 있음.
- K-fold-cross-validation
    - 데이터를 k분할하여 교차검증하는 것.
    - 데이터를 분할하고 학습하면 운이 나쁠 시 특정 방향으로만 일반화하게 학습할 수 있음
    - 모든 데이터에 대해 학습 / 테스트를 하면 그런 단점을 보완할 수 있음
- 모델 학습
    - 데이터로부터 모델을 학습하고, 이를 통해 예측 및 결정을 할 수 있는 기술의 집합
    - y = f(x)에서 함수 f(x)는 모델, f를 알아내는 과정을 학습, f로부터 y를 얻는 과정을 예측
    - 모델 학습은 데이터 수집 → 모델 선택 → 학습 → 예측(추론)으로 구성된다.

## 퍼셉트론

- 입력층 : 입력값을 받아들이는 층
- 출력층 : 퍼셉트론의 출력값을 나타내는 층 (1 또는 0)
- 노드 : 입력값의 가중합과 활성화를 결정하는 층
    - 두 입력값에 각각 가중치를 곱해 더한 뒤, 편향을 더하고 활성화 함수 통과
    - 퍼셉트론을 하나만 사용하면 단층 신경망, 여러 개 사용하면 다층 신경망
- 활성화 함수
    - 임계값을 기준으로 노드의 출력값을 결정. 층을 쌓는 효과가 생긴다.
    - 퍼셉트론에서 출력값을 다음 퍼셉트론으로 넘기는 과정에 변환 적용하는 함수
    - 모델의 복잡도를 높이기 위해 사용한다.
    - 여러 함수들이 있는데, 대표적으로 **계단(Step)**, **시그모이드**와 **ReLU**가 있다.
- 결정 경계
    - 가중치 합 + 편향 = w1x1 + w2x2 + … + b
    - 퍼셉트론을 돌려 나온 값을 그래프에 적용해보면 **대각선**이 생긴다.
    - 이 대각선보다 작은 값은 0으로 분류하고, 큰 값은 1로 분류하게 된다.
    - 여기서 대각선, 모델이 데이터를 두 클래스로 나누는 경계 자체를 결정 경계라고 함
    - 활성화 함수의 형태가 결정경계의 형태를 변경하지 않는다.
- 단층 퍼셉트론은 한 개의 결정 경계만 지니기 때문에 선형적으로 분리 가능한 문제만 적합하지만 XOR같은 비선형 문제의 경우엔 단순히 하나의 직선으로는 구분할 수 없다.

## 다층 퍼셉트론

- 다층 퍼셉트론은 여러 개의 은닉층을 포함할 수 있으며 각 은닉층은 여러 개의 뉴런으로 구성될 수 있다.
- 이 각각의 뉴런은 입력에 대해 독립적인 가중치와 편향을 가지고 각각의 결정 경계를 형성한다.
- 그래서 다층 퍼셉트론은 여러 개의 결정 경계를 가질 수 있고, 비선형 문제를 해결 가능하다.
- 각 노드에 서로 다른 가중치와 편향을 적용해 값이 나오고, 중간값과 편향을 사용해 최종 출력을 더한다. 입력 층에 0 0 / 0 1 / 1 0 / 1 1을 넣어 XOR 문제를 해결할 수 있다.

### 손실 함수

- Loss Function는 실제로 모델이 예측한 결과와 정답 사이의 오차를 측정하는 함수이다.
- 나온 결과값에 미분 연산을 통해서 기울기를 계산하고, 기울기가 크면 오차가 큰 것읻 .
- 이 함수를 사용해 모델의 성능을 평가하고, 모델의 가중치를 조정한다.
- 가중치가 조절되면, 결정 경계 또한 변하게 된다.
- 일반적으로 오차가 적은 경계가 더 좋은 결정 경계이다.
- 초기에는 MSE, **평균 제곱 오차** 손실 함수가 널리 쓰였다.
    - 회귀 문제에서 사용되며, 단순 이진 분류일 때 쓰인다.
    - 모델 예측 값과 실제 값 사이의 제곱 차이의 평균을 계산한다.
- 이후엔 교차 엔트로피 (Cross-Entropy) 손실함수가 쓰였다.
    - 분류 문제에서 사용되며, 단순 이진 분류가 아닐 때 쓰인다.
    - 실제 데이터의 확률 분포와 학습 모델이 계산한 확률 분포의 차이를 최소화하는 방식이다.
    - 모델의 예측이 실제 클래스 레이블을 얼마나 잘 반영하는지를 측정한다.
- 이 함수 리턴값을 최소화함으로서 모델은 데이터에 잘 맞도록 조정되며, 최적의 결정경계를 찾게 된다.

### 엔트로피와 크로스 엔트로피

- 엔트로피 : 확률 분포 P(x)에 대한 불확실성의 척도
    - 정보량이 낮다 = 특별하지 않다 = 엔트로피가 낮다 = 구분이 쉽다
    - 정보량이 높다 = 특별하다 = 엔트로피가 높다 = 구분이 어렵다
    - 어떤 사건의 발생 가능성이 매우 예측 가능하다면 (확률이 높다면) 그 사건의 엔트로피는 낮다. 반대로 발생 가능성이 매우 예측이 어렵다면 그 사건의 엔트로피는 높다.
    - 만약 앞뒷면이 50%로 정확히 공정한 동전을 던지게 되면, 이는 예측하기 가장 어려우며 정보량도 가장 높다. 공식을 사용하면 1비트가 나오며, 엔트로피가 제일 높다.
    - 그러나 편향되어 앞면이 나올 확률이 90%인 동전이 있다면 예측이 더 쉬워지므로 엔트로피가 낮다. 공식을 사용햐면 약 0.47비트가 나오며, 엔트로피가 비교적 낮다.
- 크로스 엔트로피 : 실제 정답 분포와 모델이 예측한 분포간 차이
    - CE가 낮다 = 두 분포의 차이가 적다 = 모델이 실제 답을 잘 예측한다.

## 최적 해 계산

- **경사 하강법**과 **오차 역전파**를 통해 최적의 값을 찾는다.
- **순전파 → 손실 계산 → 역전파(그라디언트 계산 포함)** 순으로 진행된다.

```bash
순전파 : 입력층부터 출력층까지 순차적으로 각 층의 뉴런을 통과하며 최종 출력을 계산하는 과정
```

### 오차 역전파

- 정답과 신경망이 예측한 값과의 오차를 최소화하는 가중치를 찾는 알고리즘.
- 미분의 연쇄 법칙을 이용해 출력층에 가까운 가중치부터 수정해나간다.
    - 결론적으로 신경망의 모든 가중치를 조정한다.
- 가중치에 손실 함수에서 나온 기울기(오류의 차이)값을 빼주면 새로운 가중치 값 획득
- 순전파를 통해 얻은 값과 실제 값 차이를 손실 함수로 계산, 이 값을 최소화하는 것이 목표
- 비선형 구조 활성화 함수가 필요한데, 선형 구조면 미분 결과 기울기가 아닌 상수를 출력함.

```bash
그라디언트 : 각 가중치에 대한 손실 함수의 미분값들을 벡터로 표현한 정보
손실 함수만으로는 가중치를 어떻게 변경해야 할지 정보가 담겨져 있지 않은데,
이 그라디언트를 손실 함수의 값으로 계산해내면 가중치를 최적화할 정보가 된다.

그라디언트 계산 : 손실 함수의 결과를 바탕으로 출력층 -> 입력층 거꾸로 가며
각 뉴런의 가중치에 대한 손실 함수의 그라디언트(미분값)를 계산한다.
각 층의 가중치가 손실에 얼마나 영향을 미쳤는지를 평가한다.
그라디언트(기울기)를 계산하고, 그라디언트가 가리키는 방향의 반대로 가중치를 조정해야 함

가중치 업데이트 : 계산된 그라디언트를 사용해 각 가중치를 조정한다.
이 때 학습률(Learning Rate)을 곱하여 그라디언트의 크기를 조절한다.
```

### 경사 하강법

- 시작점부터 최적화된 가중치를 얻기 위해 출발하는 학습 여정
- 함수의 기울기(경사)를 구하고, 경사의 반대 방향으로 계속 이동시키며 최솟값에 이를 때까지 반복하는 학습 방법
- 시작점부터 다음 지점까지 가는 거리를 보폭이라고 함
    - 보폭은 **학습률(Learning Rate)**과 기울기의 곱으로 표현된다.
    - 즉, 기울기가 클 수록 보폭도 커지고, 기울기가 작으면 보폭도 작아진다.
    - 학습률이 너무 크면 보폭이 너무 커버려서 수렴 속도가 느려진다.
    - 학습률이 너무 작으면 보폭이 너무 작아버려서 수렴 속도가 느려진다.
    - 적당한 학습률을 설정해야 수렴 속도가 최적화된다.
- 경사 하강법의 과정이자 AI 학습  과정
    1. 초기화 : 가중치를 초기 값으로 설정. 무작위 값일 수도 있고, 0일 수도 있고, 특별한 전략에 따른 값일 수도 있다.
    2. 순전파 : 입력 데이터를 신경망에 통과시켜 예측 값 획득
    3. 손실 계산 : 예측 값과 실제 데이터의 차이를 손실 함수를 통해 계산
    4. 그라디언트 계산 : 오차 역전파를 통해 각 가중치에 대한 손실 함수의 그라디언트를 계산
    5. 가중치 업데이트 : 계산된 그라디언트를 사용해 각 가중치를 업데이트. 이 때 학습률을 곱해 가중치 업데이트의 크기를 조절
    6. 위 과정을 손실이 충분히 작아질 때까지 반복한다.

# 3주차

## 인공지능 분류모델 RNN

- Recurrent Neural Network
- **시계열 데이터**나 **순차 데이터**를 분류하는 대표 모델
    - 시계열 데이터 : 텍스트, 음성, 생체신호 등 연속적인 형태로 존재하는 데이터
    - 즉 Classification(분류)와 Regression(회귀)를 하는 분류 모델
- 네트워크 내에서 순환하는 연결 구조를 갖고 있어 시간에 따라 변화하는 데이터의 내부 상태를 유지할 수 있다. 이 특성 덕분에 이전 정보를 기억하고, 현재 입력과 결합해 출력을 생성한다.
    - 연속학습의 일종이기 때문에, 장기 의존성 문제로 초기 정보를 후반부에 잊어버린다.
- 하나의 Unit에 반복적으로 접근하기에, 데이터의 길이가 길어지면 초기 입력 정보를 잊어버린다.
- LSTM과 GRU가 등장해 기억력 문제를 어느정도 해결했으며, 잃어버릴 메모리와 기억할 메모리를 구분하여 반복 사용한다.
- 여러 계층 활용 시 동일한 유닛에 반복 접근하며, 분류 단계에선 마지막에 MLP를 사용한다.

## 인공지능 분류모델 CNN

- Convolution Neural Network
- **이미지 분류**의 대표 모델
- 컨볼루션 연산(합성곱)으로 이루어진 합성곱 층과 활성화 함수 등으로 이루어짐
- 분류 시에는 마지막에 MLP 구조를 활용
- 입력 → 합성곱 → ReLU → 합성곱 → ReLU → 맥스풀링 → 출력

### 커널, 필터

- 커널은 이미지로부터 특징을 추출하기 위해 가중치를 행렬로 나타낸 것
- 커널의 집합을 필터라고도 부름
- 커널의 이동거리는 **스트라이드**라고 함
- 커널의 합성곱 결과로부터 얻어지는 이미지는 **특징 맵**이라고 함
- 커널의 크기만큼 있는 이미지를 한 픽셀의 특징으로 압축하는거라, 이미지가 작아짐

### 지역특징과 전역특징

- CNN 모델의 앞 부분에서 추출한 특징일 수록, **지역 특징정보**를 학습
- CNN 모델의 뒷 부분에서 추출한 특징일 수록, **전역 특징정보**를 학습
- 앞 부분은 화질이 좋아 세부적인 특징을 보고, 뒷 부분은 화질이 흐려 전체적인 모습을 본다.

### 차원 축소 (Pooling)

- 차원을 축소한다는 것은, 중요 특징만을 남겨두고 이미지 크기를 줄이는 것
- 주로 활용되는 풀링은 **Max Pooling**
    - 정의된 커널 내에서, 가장 큰 값만 남기기
- 이 외에도 평균, 최소 풀링이 존재하기는 함

### 평탄화(Flatten)

- MLP층의 입력에 이미지를 넣기 위해서는 반드시 1차원 벡터 형태여야 함
- 즉, 이미지를 **1차원 벡터로 변환**하는 작업
- tensor x.view()로 평탄화 가능

### 기타 CNN 기술 단어

- 데이터 증강 : 이미지에 여러 변형을 주어 이미지 개수를 늘리는 것. 회전, 크기 변경, 밀림, 반사, 이동 등을 사용해 서 있는 사람이나 거꾸로 매달린 사람이나 사람으로 인식하게끔 하는 것.
- 크롭핑 : 이미지의 일정 부분을 도려내기. 불필요한 부분 잘라내기
- 패딩 : 특정 영역을 0 혹은 아무 값으로 채우기. 0으로 채우는 것은 제로 패딩이라고 함
- 플립 : 대칭 회전이라고도 함. 이미지를 좌우나 상하로 뒤집는 것
- 정규화 : 데이터의 분포를 정규분포의 형태로 바꿔주는 것.
    - 어느 특정 이미지가 특정 색깔이 많다면, 해당 이미지의 특징보다 색에 집중해버릴 수 있어서 RGB 분포를 전부 똑같게 바꿔주는 작업을 의미한다.
- 오버피팅 : 머신러닝, 딥러닝에서 자주 발생하는 문제. 모델이 학습 데이터에 너무너무 잘 맞춰져있어 학습 데이터는 정답률이 매우 높지만 새로운 데이터나 검증 데이터는 성능이 좋지 않은 현상을 의미한다. 즉, 범용성이 떨어진다.
- 드롭아웃 : 학습 과정에서 네트워크의 일부 연결을 임의로 끊어서 모델이 특정 노드에 과도하게 의존하는 것을 방지하는 것.

## 인공지능 분류모델 Transformer

- Attention is all you need (NeurlPS 2017) 논문에서 첫 발표
- 시계열 데이터의 **순서정보와 무관**하게, 일괄적으로 정보를 처리할 수 있는 구조
    - Positional Encoding (PE) : 순서정보와 상관 없이 한 번에 입력을 받는 것
- **인코더 → 디코더**의 구조로 이루어져 있음
    - 인코더는 주로 분류에 활용되고, 디코더는 주로 생성에 활용됨
- 이미지 분류 시에도, 지역/전역 특징에 대해서 구분 성능이 뛰어난 Vision Transformer(ViT)

## 여러 인공지능 모델

### 인공지능 인식모델

- 가짜 얼굴인식, 스피치 인식 등 작업별 기술이 개별적으로 발전
- 스피치 인식(ASR)의 경우, **트랜스포머의 인코더만 활용하는 BERT** 기반 모델이 증가

### 인공지능 분할모델

- 이미지 분할(Segmentation)은 도로의 객체 분할, 이미지 편집 분야에 활용됨
- 동영상 객체 분할은 연속적인 프레임 내에서 동일객체 분리를 위한 작업
- 인공지능 분할 모델엔 SAM이나 Detectron2 등이 있음
    - SAM의 이미지 인코더는 ViT이다.

### 인공지능 생성모델

- GANs : 생성자와 감별자의 적대적 학습 기반 생성기술
    - 생성자가 최대한 가짜 이미지를 반들고, 감별자는 진짜와 가짜를 최대한 구별해냄
    - 감별자는 진짜 이미지가 1이 나오도록, 가짜 이미지가 0이 나오도록 학습
    - 생성자는 가짜 이미지를 생성해서 감별자의 출력이 1이 나오도록 학습
- Stable Diffusion : 이미지 ↔ 노이즈간 단계적 변환을 통한 생성기술
    - 음성, 이미지, 텍스트 데이터 모두에 적용 가능
    - Forward Diffusion Process : 이미지에 노이즈를 추가하는 학습
    - Reverse Diffusion Process : Noise로부터 데이터를 복원하는 과정 학습
- 인공지능 생성 모델이 얼마나 진짜같은지 평가하는 것은 FID
    - Frechlet Inception Distance
    - 원본 이미지 데이터 셋과 생성된 이미지 데이터 셋 사이의 통계적 거리 측정

# 4주차

- 모델 학습 과정
    1. 데이터 Tensor로 변환
    2. 그라디언트 초기화
    3. 순전파
    4. 손실함수 계산
    5. 역전파
    6. 가중치 업데이트
- DNN 모델로 수입 예측 실습
- LSTM 모델로 문장의 감정 예측 실습
- YOLO 모델로 이미지에서 객체 검출 실습
- CNN 모델로 이미지 분류

# 5주차

- CNN 모델은 컨볼루전, ReLU, Pool, Linear 레이어 등으로 구성됨
- **컨볼루전 출력 크기 = (이미지 크기 - 커널 크기 + 2 패딩) / 스트라이드 + 1**
- 컬러 이미지는 입력 데이트의 채널 수가 3

## 이미지 세그멘테이션

- Classifiaction : 이미지 안에 어떤 객체가 있는지 분류
- Object Detection : 이미지 안에 어떤 객체가 있는지 분류하고, 박스를 쳐서 위치 표현
- Semantic Segmentation : 이미지 안의 객체를 픽셀 단위로 영역을 칠하기
- Instance Segmentation : 동일한 객체라도 별도의 개체로 인식하고 다른 레이블을 할당
    - 여러 사람을 구분할 수 있다.
- Panoptic Segmentation : 픽셀 단위로, 카테고리와 객체 간 구분을 진행하는 것
    - Semantic + Instance 상위 호환
- 의료 영상에서 분할하거나, 자율 주행 환경 영상 분석에 활용하거나, 농업에 활용

## 이미지 분할 기법

- 전통적인 이미지 분할 과정은 **이진화 → 모폴로지 연산**
    - 이진화 : 특정 값을 기준으로 검정 / 하양 이미지로 분할
        - 대표적 방법인 OTSU 기법으로 이진화를 적용함
    - 모폴로지 연산 : Erode, Dilate, Open, Close로 영역 확대 축소 보정

### 모폴로지 연산

- 침식 (Erode) : 정해진 픽셀 영역을 축소하는 것
- 팽창 (Dilate) : 정해진 픽셀 영역을 확대하는 것
- Open (침식 + 팽창) : 이미지 외부의 노이즈들을 제거
- Close (팽창 + 침식) : 이미지 내부의 노이즈들을 제거

## 딥러닝 기반 이미지 분할 모델

- One-Stage : 이미지를 한 번에 분석. 속도가 빠르나 정밀도가 다소 떨어짐
- Two-Stage : 이미지에서 객체를 탐지 후, 분할하는 두 단계로 세밀하고 정확도가 높으나 속도 느림
- Mask R-CNN : 이미지에 대해 관심 영역을 추출하고, 세그멘테이션을 하는 Two-Stage CNN
- Detectron2 : Two-Stage 방식의 Panoptic 세그멘테이션 모델

# 6주차

## GAN

- Generative Adversarial Network
- 생성자 (Generator)는 노이즈(랜덤한 값을 가진 벡터)로부터 목표 이미지를 생성
- 감별자 (Discriminator)는 이미지가 입력되면 진짜, 가짜 여부를 판별
- 생성자는 감별자가 진짜라고 판단하는 이미지를 생성하기 위해 학습하고, 감별자는 가짜 이미지와 진짜 이미지를 잘 구별하도록 학습함
- **생성자**는 -1 ~ 1 범위에 해당하는 이미지 픽셀 값을 생성하기 위해 **Tanh** 사용
- **감별자**는 가짜(0)와 진짜(1)를 구분하기 위해 **Sigmoid**를 사용, Linear 사용
- Mode Collapse : GAN 학습 중, 어느 시점부터 학습이 불가능해지는 현상
    - 생성자가 하나의 데이터에 치우친 결과물만 생성하는 현상
    - 그 결과물을 만들어내면 감별자가 통과시켜주니까, 그것만 만들어버리는 것

## DCGAN

- 컨볼루전 연산을 GAN 모델에 적용한 기술
- Pytorch에선 nn.ConvTranspose2d 함수로 사용
    - 합성곱의 반대 과정을 수행하는 역합성곱, 작은 데이터를 큰 데이터로 업샘플링
    - 첫 번째 파라미터로 입력 특징 맵 채널 수
    - 두 번째 파라미터로 출력 특징 맵 채널 수
- nn.BatchNorm2d 함수로 배치 정규화를 함
- 학습 목표는 GAN과 동일
- 생성자는 업스케일링을 하기 때문에 1픽셀 짜리를 커널사이즈 7로 합성곱하면 7x7 이미지 생성
    - **출력 이미지 변 = (입력 이미지 변 - 1) * 스트라이드 - 2 패딩 + 커널 사이즈**

## Pix2Pix

- Paired 데이터셋에 대해 이미지 → 이미지 변환을 적용하는 모델
- 반드시 쌍을 이루는 데이터가 있어야 서로끼리 변환을 해줄 수 있음

## Cycle GAN

- 입출력 이미지 데이터가 Pair로 존재하지 않을 때 변환을 적용하는 기술
- A → B → A가 입출력이 같도록 학습해 Unpaired 문제를 해결

---

# 코드 실습

## 공식

- Conv2d()
    - (입력 크기 - 커널 + 2패딩) / 스트라이드 + 1
- ConvTranspose2d()
    - (입력 크기 - 1) * 스트라이드 - 2패딩 + 커널

## Numpy

```python
np.arrange(10) # 0부터 10까지 연속값 배열 생성
np.zeros(dim) # 0으로 채워진 배열 생성
np.ones(dim) # 1로 채워진 배열 생성
np.full(dim,val) # val로 채워진 배열 생성
np.array(vals) # 사용자 지정 값의 배열 생성

arr.shape # 배열의 형태 확인
arr.ndim # 배열의 차원 확인
```

## Pytorch

```python
import torch

torch.tensor() # 텐서 생성
torch.rand() # 무작위 값이 채워진 텐서 생성
torch.zeros() # 0으로 채워진 텐서 생성
torch.ones() # 1로 채워진 텐서 생성
x.view() # shape 변경, 보통 평탄화 작업 때 많이 사용

tensor[:,0] # 모든 행의 첫 번째 열
tensor[0,:] # 첫 번째 행
tensor[0,0] # 첫 번째 행의 첫 번째 열
```

## MaxPooling

```python
import numpy as np

# 가상의 이미지 데이터 생성 (4x4 픽셀, 흑백)
image = np.random.rand(4, 4)

# 1. Max Pooling
def max_pooling(image, pool_size):
  output_size = np.array(image.shape) // pool_size
  pooled_image = np.zeros(output_size)
  for i in range(output_size[0]):
    for j in range(output_size[1]):
      pooled_image[i, j] = np.max(image[i*pool_size:(i+1)*pool_size, j*pool_size:(j+1)*pool_size])
  return pooled_image

pooled_image = max_pooling(image, 2)

print(image)
print(pooled_image)
```

## 패딩, 크롭핑

```python
import numpy as np

image = np.random.rand(4, 4)

# 2 & 3. Padding (Zero-padding)
def padding(image, pad_width):
    padded_image = np.zeros((image.shape[0] + 2 * pad_width, image.shape[1] + 2 * pad_width))
    padded_image[pad_width:-pad_width, pad_width:-pad_width] = image
    return padded_image

# 4. Crop
def crop(image, start_row, start_col, height, width):
    return image[start_row:start_row+height, start_col:start_col+width]

# 적용 예시
padded_image = padding(image, 1)
cropped_image = crop(image, 1, 1, 2, 2)

print("Original Image:")
print(image)
print("\nPadded Image:")
print(padded_image)
print("\nCropped Image:")
print(cropped_image)
```

## Flip

```python
import numpy as np

image = np.random.rand(4, 4)

def flip_image(image, axis):
  # Assuming image is a 2D or 3D numpy array
  if axis == 'horizontal':
    # Flip horizontally
    return image[:, ::-1]
  elif axis == 'vertical':
    # Flip vertically
    return image[::-1, :]
  else:
    raise ValueError("Axis must be 'horizontal' or 'vertical'")

horizontal_image = flip_image(image, 'horizontal')
vertical_image = flip_image(image, 'vertical')

print("Original Image:")
print(image)
print("horizontal_image:")
print(horizontal_image)
print("vertical_image:")
print(vertical_image)
```

## 평탄화, 정규화

```python
import numpy as np

image = np.random.rand(4, 4)

# 6. Flatten
def flatten(image):
  return image.flatten()

# 7. Normalization
def normalize(image):
  return (image - np.min(image)) / (np.max(image) - np.min(image))

# 적용 예시
flattened_image = flatten(image)
normalized_image = normalize(image)

print(image)
print(flattened_image)
print(normalized_image)
```

## 퍼셉트론

```python
import torch
import torch.nn as nn
import torch.optim as optim
# 퍼셉트론 모델 정의
class Perceptron(nn.Module):
	def __init__(self, input_size):
		super(Perceptron, self).__init__()
		self.linear = nn.Linear(input_size, 1) # 입력 크기에서 1개의 출력으로 매핑
		
	def forward(self, x):
		return torch.sigmoid(self.linear(x)) # 활성화 함수로 시그모이드 사용
		
# AND 연산을 위한 학습 데이터와 레이블
X = torch.tensor([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])
y = torch.tensor([[0.], [0.], [0.], [1.]])

# 모델, 손실 함수, 최적화 알고리즘 설정
model = Perceptron(2) # 입력 크기는 2
criterion = nn.BCELoss() # 이진 분류를 위한 Binary Cross-Entropy 손실
optimizer = optim.SGD(model.parameters(), lr=0.1) # 확률적 경사 하강법

# 학습
epochs = 1000
for epoch in range(epochs):
	optimizer.zero_grad() # 그라디언트 초기화
	outputs = model(X) # 순전파
	loss = criterion(outputs, y) # 손실 계산
	loss.backward() # 역전파
	optimizer.step() # 가중치 업데이트
	
	if (epoch+1) % 100 == 0:
		print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')
		
# 학습된 모델 테스트
with torch.no_grad(): # 기울기 계산을 수행하지 않음
	y_predicted = model(X)
	y_predicted_cls = y_predicted.round() # 확률을 0 또는 1로 반올림
	accuracy = y_predicted_cls.eq(y).sum() / float(y.shape[0])
	print(f'Accuracy: {accuracy:.4f}')
```

## MLP

```python
import torch
import torch.nn as nn
import torch.optim as optim

# MLP 모델 정의
class MLP(nn.Module):
  def __init__(self, input_size):
    super(MLP, self).__init__()
    self.hidden = nn.Linear(2, 4) # 은닉층
    self.activation = nn.Sigmoid()
    self.output = nn.Linear(4, 1) # 은닉층에서 출력

  def forward(self, x):
    x = self.hidden(x)
    x = self.activation(x)
    x = self.output(x)
    x = self.activation(x)
    return x

# XOR 연산을 위한 학습 데이터와 레이블
X = torch.tensor([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])
y = torch.tensor([[0.], [1.], [1.], [0.]])

# 모델, 손실 함수, 최적화 알고리즘 설정
model = MLP(2) # 입력 크기는 2
criterion = nn.BCELoss() # 이진 분류를 위한 Binary Cross-Entropy 손실
optimizer = optim.SGD(model.parameters(), lr=0.5) # 확률적 경사 하강법

# 학습
epochs = 1000
for epoch in range(epochs):
  optimizer.zero_grad() # 그라디언트 초기화
  outputs = model(X) # 순전파
  loss = criterion(outputs, y) # 손실 계산
  loss.backward() # 역전파
  optimizer.step() # 가중치 업데이트

  if (epoch+1) % 100 == 0:
    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')

# 학습된 모델 테스트
with torch.no_grad(): # 기울기 계산을 수행하지 않음
  y_predicted = model(X)
  y_predicted_cls = y_predicted.round() # 확률을 0 또는 1로 반올림
  accuracy = y_predicted_cls.eq(y).sum() / float(y.shape[0])
  print(f'Accuracy: {accuracy:.4f}')
```

## LSTM

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

def encode_sentence(data):
    sentences = [s[0].split() for s in data]
    labels = [s[1] for s in data]
    max_len = max(len(s) for s in sentences)
    for i in range(len(sentences)):
        while len(sentences[i]) < max_len:
            sentences[i].append("<PAD>")
    vocab = set(word for sentence in sentences for word in sentence)
    word_to_idx = {word: idx for idx, word in enumerate(vocab)}
    indexed_sentences = [[word_to_idx[word] for word in sentence] for sentence in sentences]
    return indexed_sentences, labels, vocab

data = [
    ("This is great", 1),
    ("I love it", 1),
    ("So good", 1),
    ("Amazing work", 1),
    ("Fantastic job", 1),
    ("Not good", 0),
    ("I am disappointed", 0),
    ("Bad experience", 0),
    ("Not happy with this", 0),
    ("Could be better", 0),
    ("Amazing work", 1),
    ("Fantastic job", 1),
    ("Not happy with this", 0),
    ("Could be better", 0),
    ("I am very good", 1),
    ("An excellent experience", 1),
    ("I am not happy", 0),
    ("It was a bad day", 0),
]

class SimpleLSTM(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):
        super(SimpleLSTM, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, text):
        embedded = self.embedding(text)
        output, (hidden, _) = self.lstm(embedded)
        hidden = hidden.squeeze(0)
        out = self.fc(hidden)
        return out

# 학습/테스트 데이터 생성
indexed_sentences, labels, vocab = encode_sentence(data)
inputs = torch.tensor(indexed_sentences, dtype=torch.long)
targets = torch.tensor(labels, dtype=torch.float32)

inputs_train, inputs_test = inputs[:10], inputs[10:]
targets_train, targets_test = targets[:10], targets[10:]

# 모델 초기화
model = SimpleLSTM(len(vocab), embedding_dim=8, hidden_dim=4, output_dim=1)
loss_fn = nn.BCEWithLogitsLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.5)

# 학습 루프
for epoch in range(3000):
    model.train()
    optimizer.zero_grad()
    outputs = model(inputs_train)
    loss = loss_fn(outputs.squeeze(), targets_train)
    loss.backward()
    optimizer.step()
    if epoch % 10 == 0:
        print(f'Epoch {epoch}, Loss: {loss.item()}')

model.eval()
with torch.no_grad():
    predictions = model(inputs_test)
    predictions = torch.sigmoid(predictions).squeeze()
    predicted_labels = predictions.round()

    accuracy = (predicted_labels == targets_test.squeeze()).float().mean()
    print(f'Test Accuracy: {accuracy.item()}')

    print("Test Predictions:")
    for i, (sentence, _) in enumerate(data[10:]):
        print(f"{sentence} - Predicted: {'Positive' if predicted_labels[i] == 1 else 'Negative'}")
```

## CNN

```python
import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# 데이터 전처리를 위한 변환 정의
transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Resize((16, 16)),
     transforms.Normalize((0.5,), (0.5,))])

# 학습 데이터셋 로딩
trainset = torchvision.datasets.FashionMNIST(root='./data', train=True,
                                        download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True)

# 테스트 데이터셋 로딩
testset = torchvision.datasets.FashionMNIST(root='./data', train=False,
                                       download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False)

# 클래스 이름
classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 6, 5) # 28x28 -> 24x24
        self.pool = nn.MaxPool2d(2, 2) # 24x24 -> 12x12, 8x8 -> 4x4
        self.relu = nn.ReLU()
        self.conv2 = nn.Conv2d(6, 16, 5) # 12x12 -> 8x8
        self.fc1 = nn.Linear(16 * 1 * 1, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10) # 10개 클래스 분류
    def forward(self, x):
        x = self.pool(self.relu(self.conv1(x)))
        x = self.pool(self.relu(self.conv2(x)))
        x = x.view(-1, 16 * 1 * 1)
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        return x

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
net = Net().to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

for epoch in range(2):  # 데이터셋을 여러 번(10 epochs) 반복
    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        inputs, labels = data[0].to(device), data[1].to(device)
        optimizer.zero_grad()
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
        if i % 2000 == 1999:
            print(f'[{epoch + 1}, {i + 1}] loss: {running_loss / 2000:.3f}')
            running_loss = 0.0

print('Finished Training')

correct = 0
total = 0
# 기울기를 계산하지 않아도 되므로, 메모리 소비를 줄이고 계산 속도를 높이기 위해 no_grad() 사용
with torch.no_grad():
    for data in testloader:
        images, labels = data[0].to(device), data[1].to(device)
        outputs = net(images)
        # 가장 높은 값(energy)을 갖는 분류(class)를 선택합니다.
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print(f'Accuracy of the network on the 10000 test images: {100 * correct // total}%')

```

## DNN

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset

# DNN 모델 정의
class DNN(nn.Module):
    def __init__(self):
        super(DNN, self).__init__()
        self.layer1 = nn.Linear(14, 128) # 데이터가 14개의 속성을 지니므로 입력층 노드 14개로 설정
        self.layer2 = nn.Linear(128, 64) # 은닉층 노드를 128 -> 64로 많게 설정해 문제 해결력 향상 및 복잡도 감소
        self.layer3 = nn.Linear(64, 2) # 소득이 50K보다 적은가, 많은가를 판별하기 위해 출력층 노드는 2개

    def forward(self, x):
        # ReLU 비선형성을 도입한 활성화 함수를 적용
        x = torch.relu(self.layer1(x))
        x = torch.relu(self.layer2(x))
        x = self.layer3(x)
        return x

# 모델, 손실 함수, 최적화 알고리즘 초기화
model = DNN().to(device)
criterion = nn.CrossEntropyLoss() # 크로스 엔트로피 손실 함수 사용
optimizer = optim.Adam(model.parameters(), lr=0.002) # 최적화 알고리즘으로 Adam 사용, Learning Rate 0.0002 설정

# 학습 함수와 루프
def train(model, train_loader, criterion, optimizer, epochs):
    model.train() # 학습 모드로 설정
    for epoch in range(epochs):
        total_loss = 0
        for features, labels in train_loader:
            optimizer.zero_grad()
            outputs = model(features)
            loss = criterion(outputs, labels) # 손실 계산
            loss.backward() # 역전파
            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(train_loader) # 평균 손실 계산
        print(f"Epoch {epoch+1}, Loss: {avg_loss:.10f}") # 손실률을 소수점 뒤 10자리까지 출력

# 테스트 함수
def test(model, test_loader):
    model.eval() # 평가 모드로 설정
    correct = 0
    total = 0
    with torch.no_grad():
        for features, labels in test_loader:
            outputs = model(features)
            _, predicted = torch.max(outputs.data, 1) # 최대 확률 클래스 선택
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    accuracy = 100 * correct / total
    print(f"Accuracy: {accuracy:.10f}%") # 정확도를 뒤 10자리까지 출력

# 학습 및 테스트 실행
train(model, train_loader, criterion, optimizer, 20) # epoch 25회 설정
test(model, test_loader)

```

## GAN

```python
import torch
import torchvision
import torchvision.transforms as transforms
from torch import nn
import matplotlib.pyplot as plt

# 데이터셋 로딩
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Resize(14),
    transforms.Normalize((0.5,), (0.5,)),
])

train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)

# 생성자 정의
class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()
        self.main = nn.Sequential(
            nn.Linear(100, 128),
            nn.LeakyReLU(0.2),
            nn.Linear(128, 196),
            nn.Tanh()
        )
    def forward(self, input):
        return self.main(input).view(-1, 1, 14, 14)

# 감별자 정의
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.main = nn.Sequential(
            nn.Linear(14*14, 256),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3),
            nn.Linear(256, 1),
            nn.Sigmoid()
        )

    def forward(self, input):
        input = input.view(-1, 14*14)
        return self.main(input)

# 모델 초기화
G = Generator().cuda()
D = Discriminator().cuda()

# 옵티마이저 설정
optimizerG = torch.optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))
optimizerD = torch.optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))

# 손실 함수 설정
criterion = nn.BCELoss()

# 학습 시작
epochs = 30
for epoch in range(epochs):
    for i, (images, _) in enumerate(train_loader):
        images = images.cuda()
        # 실제 데이터를 사용하여 판별자 학습
        D.zero_grad()
        real_labels = torch.ones(images.size(0), 1).cuda()
        real_output = D(images)
        d_loss_real = criterion(real_output, real_labels)

        # 가짜 데이터 생성하여 판별자 학습
        noise = torch.randn(images.size(0), 100).cuda()
        fake_images = G(noise)
        fake_labels = torch.zeros(images.size(0), 1).cuda()
        fake_output = D(fake_images.detach())
        d_loss_fake = criterion(fake_output, fake_labels)

        # 판별자 손실 계산 및 업데이트
        d_loss = d_loss_real + d_loss_fake
        d_loss.backward()
        optimizerD.step()

        # 생성자 학습
        G.zero_grad()
        output = D(fake_images)
        g_loss = criterion(output, real_labels)
        g_loss.backward()
        optimizerG.step()

    print(f'Epoch [{epoch+1}/{epochs}], d_loss: {d_loss.item():.4f}, g_loss: {g_loss.item():.4f}')

    with torch.no_grad():
        test_z = torch.randn(16, 100).cuda()
        generated_images = G(test_z)

        # 이미지 그리드 생성
        grid = torchvision.utils.make_grid(generated_images, nrow=4, normalize=True)
        plt.figure(figsize=(10,10))
        plt.imshow(grid.permute(1, 2, 0).cpu().numpy())
        plt.axis('off')

        # 생성된 이미지 저장
        plt.savefig(f'sample_data/epoch_{epoch+1}.png')
        plt.close()

# 학습된 모델로부터 이미지 생성
with torch.no_grad():
    test_noise = torch.randn(16, 100).cuda()
    generated_images = G(test_noise)
    generated_images = generated_images.cpu()
    for i in range(16):
        plt.subplot(4, 4, i+1)
        plt.imshow(generated_images[i].reshape(14, 14), cmap='gray')
        plt.axis('off')
    plt.show()
```

## DCGAN

- Conv 출력 사이즈 = (입력 사이즈 - 커널 사이즈 + 2*패딩) / 스트라이드 + 1
- ConvTranspose 출력 사이즈 = (입력 사이즈 - 1) * 스트라이드 - 2*패딩 + 커널 사이즈

```python
# 데이터셋 로딩 (이미지 크기를 14x14로 리사이즈)
transform = transforms.Compose([
    transforms.Resize(14),
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)

# 생성자 정의
class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()        
        self.model = nn.Sequential(
            # 1x1x100 크기의 잠재 벡터를 7x7x256 크기의 특징 맵으로 변환
            # (1-1) * 1 - 0 + 7 = 7
            nn.ConvTranspose2d(100, 256, kernel_size=7, stride=1, padding=0),
            nn.BatchNorm2d(256),
            nn.ReLU(True),
            
            # 7x7x256에서 14x14x1 크기로 업샘플링
            # (7-1) * 2 - 2 + 4 = 14
            nn.ConvTranspose2d(256, 1, kernel_size=4, stride=2, padding=1),
            nn.Tanh()
        )
    def forward(self, z):
        z = z.view(-1, 100, 1, 1)
        img = self.model(z)        
        return img

# 감별자 정의
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.model = nn.Sequential(
		        # (14 - 3 + 2) / 2 + 1 = 6.5 + 1 = 7.5 = 7
            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Dropout(0.25),
            # (7 - 3 + 2) / 2 + 1 = 4
            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Dropout(0.25),
            # (4 - 3 + 0) / 2 + 1 = 0.5 + 1 = 1.5
            nn.Conv2d(64, 1, kernel_size=3, stride=2, padding=0),
            nn.Sigmoid()
        )

    def forward(self, img):
        validity = self.model(img)
        return validity.view(-1, 1)
```

```python
# 데이터셋 로딩 (이미지 크기가 28x28)
transform = transforms.Compose([
    # transforms.Resize(14),
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)

# 생성자 정의
class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()
        self.model = nn.Sequential(
            # 1x1x100 크기의 잠재 벡터를 7x7x256 크기의 특징 맵으로 변환
            # (1-1) * 1 - 0 + 7 = 7
            nn.ConvTranspose2d(100, 256, kernel_size=7, stride=1, padding=0),
            nn.BatchNorm2d(256),
            nn.ReLU(True),

            # 7x7x256에서 14x14x1 크기로 업샘플링
            # (7-1) * 2 - 2 + 4 = 14
            nn.ConvTranspose2d(256, 256, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(True),

            # 14x14x1에서 28x28x1 크기로 업샘플링
            # (14-1) * 2 - 2 + 4 = 28
            nn.ConvTranspose2d(256, 1, kernel_size=4, stride=2, padding=1),
            nn.Tanh()
        )
    def forward(self, z):
        z = z.view(-1, 100, 1, 1)
        img = self.model(z)
        return img

# 감별자 정의
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.model = nn.Sequential(
		        # (28 - 3 + 2) / 2 + 1 = 13.5 + 1 = 14.5 = 14
            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Dropout(0.25),
            # (14 - 5 + 0) / 2 + 1 = 4.5 + 1 = 5.5 = 5
            nn.Conv2d(32, 64, kernel_size=5, stride=2, padding=0),
            nn.BatchNorm2d(64),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Dropout(0.25),
            # (5 - 4 + 0) / 2 + 1 = 0.5 + 1 = 1.5 = 1
            nn.Conv2d(64, 1, kernel_size=4, stride=2, padding=0),
            nn.Sigmoid()
        )
```